{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books"
      ],
      "metadata": {
        "id": "wiuVfhQKWObH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess text\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "file_path = \"hp_1.txt\"  # Ensure you have this file in your Colab or local directory\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "# Tokenization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(oov_token='<OOV>') # Out-Of-Vocabulary token\n",
        "                                        # If a word not seen during training appears later, it will be replaced with <OOV>\n",
        "                                        # Helps handle unknown words instead of ignoring them\n",
        "tokenizer.fit_on_texts([text]) # analyzes the input text and creates a word index (mapping of words to unique integers)\n",
        "total_words = len(tokenizer.word_index) + 1 #  0 is usually reserved for padding\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0] # converts the input text into a list of numbers based on the word index\n",
        "seq_length = 50  # Each input sequence contains 50 words\n",
        "\n",
        "# First seq_length tokens (input): Used for training the model.\n",
        "# Last token (target): Used as the label the model tries to predict.\n",
        "# so total of (50 + 1) in one input_sequence index\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "# Pad sequences and split inputs/targets\n",
        "# after this X will have inputs and y will have label for those inputs\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "# One-hot encode the labels , note- there are other ways for\n",
        "# encoding like pre-trained word2vec encoding and so on\n",
        "\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Build the Simple RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=64, input_length=seq_length),  # Word embeddings\n",
        "    SimpleRNN(256, return_sequences=False),  # RNN Layer\n",
        "    Dense(256, activation='relu'),  # Fully Connected Layer\n",
        "    Dense(total_words, activation='softmax')  # Output Layer\n",
        "])\n",
        "\n",
        "# 256 in RNN - The number of hidden units (size of the hidden state vector)\n",
        "# return_sequences=False  - The RNN will only return the final hidden state after processing the entire sequence\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=30, batch_size=128)\n",
        "\n",
        "# Function to generate text using RNN\n",
        "def generate_text(seed_text, next_words=50):\n",
        "    for _ in range(next_words):\n",
        "        tokenized_input = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        tokenized_input = pad_sequences([tokenized_input], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        predicted_probs = model.predict(tokenized_input, verbose=0)\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        predicted_word = tokenizer.index_word.get(predicted_index, \"<OOV>\")\n",
        "\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text\n",
        "\n",
        "# Generate text using the trained model\n",
        "print(generate_text(\"harry looked at\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_TyID61VPRv",
        "outputId": "a4c8e039-1d72-473c-b60c-802cb39a8b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 175ms/step - accuracy: 0.0413 - loss: 6.9476\n",
            "Epoch 2/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 174ms/step - accuracy: 0.0633 - loss: 6.2668\n",
            "Epoch 3/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 184ms/step - accuracy: 0.0958 - loss: 5.8224\n",
            "Epoch 4/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 180ms/step - accuracy: 0.1183 - loss: 5.4685\n",
            "Epoch 5/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 178ms/step - accuracy: 0.1323 - loss: 5.1892\n",
            "Epoch 6/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 176ms/step - accuracy: 0.1442 - loss: 4.9743\n",
            "Epoch 7/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 175ms/step - accuracy: 0.1536 - loss: 4.7675\n",
            "Epoch 8/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 177ms/step - accuracy: 0.1620 - loss: 4.5828\n",
            "Epoch 9/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 174ms/step - accuracy: 0.1754 - loss: 4.3532\n",
            "Epoch 10/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 179ms/step - accuracy: 0.1875 - loss: 4.1523\n",
            "Epoch 11/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 174ms/step - accuracy: 0.2044 - loss: 3.9455\n",
            "Epoch 12/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 175ms/step - accuracy: 0.2225 - loss: 3.7524\n",
            "Epoch 13/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 177ms/step - accuracy: 0.2509 - loss: 3.5518\n",
            "Epoch 14/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 178ms/step - accuracy: 0.2757 - loss: 3.3703\n",
            "Epoch 15/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 176ms/step - accuracy: 0.3001 - loss: 3.1897\n",
            "Epoch 16/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 174ms/step - accuracy: 0.3292 - loss: 3.0218\n",
            "Epoch 17/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 175ms/step - accuracy: 0.3509 - loss: 2.8634\n",
            "Epoch 18/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 179ms/step - accuracy: 0.3761 - loss: 2.7213\n",
            "Epoch 19/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 176ms/step - accuracy: 0.4011 - loss: 2.5683\n",
            "Epoch 20/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 174ms/step - accuracy: 0.4259 - loss: 2.4416\n",
            "Epoch 21/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 178ms/step - accuracy: 0.3987 - loss: 2.7152\n",
            "Epoch 22/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 177ms/step - accuracy: 0.3633 - loss: 2.7696\n",
            "Epoch 23/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 180ms/step - accuracy: 0.4381 - loss: 2.3692\n",
            "Epoch 24/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 178ms/step - accuracy: 0.4806 - loss: 2.1567\n",
            "Epoch 25/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 178ms/step - accuracy: 0.5114 - loss: 2.0148\n",
            "Epoch 26/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 176ms/step - accuracy: 0.5338 - loss: 1.9035\n",
            "Epoch 27/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 178ms/step - accuracy: 0.5536 - loss: 1.8056\n",
            "Epoch 28/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 179ms/step - accuracy: 0.5729 - loss: 1.7186\n",
            "Epoch 29/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 178ms/step - accuracy: 0.5874 - loss: 1.6394\n",
            "Epoch 30/30\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 177ms/step - accuracy: 0.6053 - loss: 1.5639\n",
            "harry looked at all when he knew this was the only one who could be allowed on the leaky cauldron they were about the same your new shock and ron and hermione were playing under the other side of the clearing all sorts of lizards and snakes were crawling and slithering over bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model learns local patterns, not long-term dependencies\n",
        "\n",
        "- RNNs struggle with long-range dependencies because they do not retain information well over long sequences.\n",
        "- This is why the text seems grammatically okay but lacks deeper context.\n",
        "\n",
        "The model generates phrases based on probabilities\n",
        "\n",
        "- It predicts the most likely next word given the past words.\n",
        "- It does not understand meaning but follows statistical patterns.\n",
        "- It captures writing style but lacks coherence\n",
        "\n",
        "Words appear logically related but do not form a strong narrative.\n",
        "The model does not truly \"understand\" the book, it just mimics word usage."
      ],
      "metadata": {
        "id": "Lt12QI0zdTp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Function to load dataset\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Load Harry Potter book text\n",
        "file_path = \"hp_1.txt\"\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text into sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "seq_length = 50  # Each input sequence will have 50 words\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "# Pad sequences and split into inputs (X) and labels (y)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)  # One-hot encode labels\n",
        "\n",
        "# LSTM Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=100, input_length=seq_length),\n",
        "    LSTM(256, return_sequences=True),  # First LSTM layer\n",
        "    LSTM(256),  # Second LSTM layer\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "\n",
        "# Function to Generate Text\n",
        "def generate_text(seed_text, next_words=50, temperature=1.0):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_probs = np.log(predicted_probs) / temperature  # Adjust randomness\n",
        "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
        "        predicted_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
        "\n",
        "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "print(generate_text(\"harry looked at\", next_words=50, temperature=0.7))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM90QPqgU8f0",
        "outputId": "31be48a7-4b50-4a5e-93bc-2079cc932eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 955ms/step - accuracy: 0.0444 - loss: 7.0546\n",
            "Epoch 2/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 952ms/step - accuracy: 0.0545 - loss: 6.3644\n",
            "Epoch 3/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 954ms/step - accuracy: 0.0850 - loss: 6.0058\n",
            "Epoch 4/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 954ms/step - accuracy: 0.1083 - loss: 5.6874\n",
            "Epoch 5/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 953ms/step - accuracy: 0.1193 - loss: 5.4484\n",
            "Epoch 6/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 960ms/step - accuracy: 0.1261 - loss: 5.2622\n",
            "Epoch 7/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 955ms/step - accuracy: 0.1343 - loss: 5.1024\n",
            "Epoch 8/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 959ms/step - accuracy: 0.1428 - loss: 4.9089\n",
            "Epoch 9/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 958ms/step - accuracy: 0.1497 - loss: 4.7659\n",
            "Epoch 10/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 961ms/step - accuracy: 0.1609 - loss: 4.5944\n",
            "Epoch 11/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 961ms/step - accuracy: 0.1632 - loss: 4.4727\n",
            "Epoch 12/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 969ms/step - accuracy: 0.1744 - loss: 4.3255\n",
            "Epoch 13/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 964ms/step - accuracy: 0.1798 - loss: 4.2055\n",
            "Epoch 14/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 957ms/step - accuracy: 0.1920 - loss: 4.0826\n",
            "Epoch 15/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 957ms/step - accuracy: 0.2041 - loss: 3.9595\n",
            "Epoch 16/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 958ms/step - accuracy: 0.2195 - loss: 3.8434\n",
            "Epoch 17/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 957ms/step - accuracy: 0.2321 - loss: 3.7398\n",
            "Epoch 18/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 956ms/step - accuracy: 0.2457 - loss: 3.6329\n",
            "Epoch 19/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 957ms/step - accuracy: 0.2586 - loss: 3.5571\n",
            "Epoch 20/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 957ms/step - accuracy: 0.2710 - loss: 3.4554\n",
            "harry looked at him he reached the receiver off harry’s broom and saw it another way from the middle seat to sit with his cold stare “‘oh harry could want to see him to keep home on the contrary hagrid put a choking noise with a soccer ball wooden malkin was going to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Function to load dataset\n",
        "def load_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Load Harry Potter book text\n",
        "file_path = \"hp_1.txt\"\n",
        "text = load_data(file_path).lower()\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text into sequences\n",
        "input_sequences = []\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "seq_length = 50  # Each input sequence will have 50 words\n",
        "\n",
        "for i in range(seq_length, len(tokens)):\n",
        "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
        "\n",
        "# Pad sequences and split into inputs (X) and labels (y)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)  # One-hot encode labels\n",
        "\n",
        "# GRU Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=100, input_length=seq_length),\n",
        "    GRU(256, return_sequences=True),  # First GRU layer\n",
        "    GRU(256),  # Second GRU layer\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "\n",
        "# Function to Generate Text\n",
        "def generate_text(seed_text, next_words=50, temperature=1.0):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_probs = np.log(predicted_probs) / temperature  # Adjust randomness\n",
        "        predicted_probs = np.exp(predicted_probs) / np.sum(np.exp(predicted_probs))\n",
        "        predicted_index = np.random.choice(range(len(predicted_probs)), p=predicted_probs)\n",
        "\n",
        "        output_word = tokenizer.index_word.get(predicted_index, \"\")\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "print(generate_text(\"harry looked at\", next_words=50, temperature=0.7))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iQbFnO0eHYj",
        "outputId": "e8ab9cdf-0383-485c-de64-cb4acbb48c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 863ms/step - accuracy: 0.0410 - loss: 7.2053\n",
            "Epoch 2/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 853ms/step - accuracy: 0.0453 - loss: 6.6918\n",
            "Epoch 3/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 853ms/step - accuracy: 0.0889 - loss: 6.0476\n",
            "Epoch 4/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 849ms/step - accuracy: 0.1282 - loss: 5.4514\n",
            "Epoch 5/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 854ms/step - accuracy: 0.1482 - loss: 4.9923\n",
            "Epoch 6/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 851ms/step - accuracy: 0.1716 - loss: 4.5540\n",
            "Epoch 7/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 853ms/step - accuracy: 0.2031 - loss: 4.1501\n",
            "Epoch 8/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 853ms/step - accuracy: 0.2495 - loss: 3.7553\n",
            "Epoch 9/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 854ms/step - accuracy: 0.3012 - loss: 3.4040\n",
            "Epoch 10/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m554s\u001b[0m 841ms/step - accuracy: 0.3478 - loss: 3.1198\n",
            "Epoch 11/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 844ms/step - accuracy: 0.3932 - loss: 2.8469\n",
            "Epoch 12/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 844ms/step - accuracy: 0.4374 - loss: 2.6017\n",
            "Epoch 13/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 839ms/step - accuracy: 0.4744 - loss: 2.3920\n",
            "Epoch 14/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 843ms/step - accuracy: 0.5166 - loss: 2.1904\n",
            "Epoch 15/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 846ms/step - accuracy: 0.5535 - loss: 1.9961\n",
            "Epoch 16/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m575s\u001b[0m 909ms/step - accuracy: 0.5943 - loss: 1.8273\n",
            "Epoch 17/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m579s\u001b[0m 842ms/step - accuracy: 0.6278 - loss: 1.6673\n",
            "Epoch 18/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 843ms/step - accuracy: 0.6612 - loss: 1.5199\n",
            "Epoch 19/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 844ms/step - accuracy: 0.6966 - loss: 1.3732\n",
            "Epoch 20/20\n",
            "\u001b[1m633/633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 843ms/step - accuracy: 0.7252 - loss: 1.2449\n",
            "harry looked at me — where they looked right on it so where it looked away looked quickly looked quickly through the other two heads where one other one for the other years — it had won no two else then they were going to him about him that you were because snape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCKpdkdHbVF7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}